export const testPostOutput =
  '\n\n**Josh Wulf**: What’s the issue? You want it to do more?\n\n**Sarath Kumar**: Yes, we have added 1000 workflows, and 200 instances per second are getting added randomly into this worflows.\nThis was running fine, until there is no transition of instances between workflows.\n\n**Sarath Kumar**: I have a use case where my instance1 while processing, will add instance to workflow2. So while doing this process from worker, we are getting the mentioned exception\n\n**Josh Wulf**: When you say “add instance”, do you mean “createWorkflowInstance”?\n\n**Sarath Kumar**: yes,\n\n**Sarath Kumar**: \n\n```\npublic long addInstance(String bpmnProcessId, Object variables) {\n    WorkflowInstanceEvent workflowInstanceEvent = (WorkflowInstanceEvent)this.zeebeClient.newCreateInstanceCommand().bpmnProcessId(bpmnProcessId).latestVersion().variables(variables).send().join();\n   \n    return workflowInstanceEvent.getWorkflowInstanceKey();\n}\n```\n\n\n**Josh Wulf**: So, are you saying that “a Worker for a Service Task in Workflow 1 creates an instance of Workflow 2”\n\n**Sarath Kumar**: yes, correct\n\n**Josh Wulf**: And how are Workflow 1 instances created?\n\n**Sarath Kumar**: They are created via separate service\n\n**Josh Wulf**: So for each workflow 1, you create one instance of workflow 2?\n\n**Sarath Kumar**: It depends on the use case. Some of the case it will end in workflow 1 itself. But for some use cases, it will create instances in any of the other desired workflow.\n\n**Sarath Kumar**: We are seeing this exception randomly, while creating this new instance from worker\n\n**Josh Wulf**: So I hear two different things\n\n**Josh Wulf**: One is an error message in the worker, appearing randomly, that the max # of parallel requests are in flight\n\n**Josh Wulf**: The other is that workflows no longer progress on the brokers\n\n**Josh Wulf**: Is that correct?\n\n**Josh Wulf**: Or by `there is no transition of instances between workflows.` do you mean that workers can no longer create new instances of workflows?\n\n**Sarath Kumar**: yes, I have added an instance to worflow1, and workflow1 trying to add new instance to workflow2, while doing this at the rate of 200req/sec, we are seeing this error ""io.zeebe.client.api.command.ClientStatusException: Reached maximum capacity of requests handled"\n\nAs the worker is throwing an error, workflow instance moves to failed state\n\n**Josh Wulf**: Uh-huh\n\n**Josh Wulf**: 3 brokers. One gateway? How many workers? What resources do the brokers have? RAM, CPU?\n\n**Josh Wulf**: Probably the gateway is saturated\n\n**Sarath Kumar**: yes, 3 brokers with 1 gateway. Let me pull the resources\n\n**Josh Wulf**: Also, what does `zbctl status` show you?\n\n**Josh Wulf**: And what is in the Gateway logs?\n\n**Josh Wulf**: That message is emitted here: <https: //github.com/zeebe-io/zeebe/blob/a417a8fee9c93856420ed59ab10f2517a925a081/broker/src/main/java/io/zeebe/broker/transport/commandapi/CommandApiRequestHandler.java#L140>\n\n**Josh Wulf**: That will tell you in the logs what partition:\n\n**Josh Wulf**: `"Partition-{} receiving too many requests. Current limit {} inflight {}, dropping request {} from gateway"`\n\n**Sarath Kumar**: yeah sure, let me pull the logs from gateway\n\n**Josh Wulf**: @Deepthi do you need to run at a log level other than INFO to see that message in the logs?\n\n**Sarath Kumar**: `2020-02-04 06:28:20.803 [] [main] INFO io.zeebe.gateway - Starting gateway with configuration {`\n "network": {\n  "host": "10.177.71.93",\n  "port": 26500,\n  "minKeepAliveInterval": "30s"\n },\n "cluster": {\n  "contactPoint": "10.177.71.226:26502",\n  "maxMessageSize": "4M",\n  "requestTimeout": "15s",\n  "clusterName": "zeebe-cluster",\n  "memberId": "gateway",\n  "host": "0.0.0.0",\n  "port": 26502\n },\n "threads": {\n  "managementThreads": 1\n },\n "monitoring": {\n  "enabled": false,\n  "host": "0.0.0.0",\n  "port": 9600\n },\n "security": {\n  "enabled": false\n }\n}\n\n**Sarath Kumar**: Above are the ones available in log., Not able to see that partition error message\n\n**Josh Wulf**: and `zbctl status?`\n\n**Sarath Kumar**: \n\n```\nCluster size: 3\nPartitions count: 16\nReplication factor: 3\nBrokers:\n  Broker 0 - 10.177.71.226:26501\n    Partition 1 : Leader\n    Partition 2 : Leader\n    Partition 3 : Follower\n    Partition 4 : Leader\n    Partition 5 : Leader\n    Partition 6 : Follower\n    Partition 7 : Leader\n    Partition 8 : Leader\n    Partition 9 : Follower\n    Partition 10 : Leader\n    Partition 11 : Leader\n    Partition 12 : Follower\n    Partition 13 : Leader\n    Partition 14 : Leader\n    Partition 15 : Follower\n    Partition 16 : Leader\n  Broker 1 - 10.177.71.39:26501\n    Partition 1 : Follower\n    Partition 2 : Follower\n    Partition 3 : Follower\n    Partition 4 : Follower\n    Partition 5 : Follower\n    Partition 6 : Follower\n    Partition 7 : Follower\n    Partition 8 : Follower\n    Partition 9 : Follower\n    Partition 10 : Follower\n    Partition 11 : Follower\n    Partition 12 : Follower\n    Partition 13 : Follower\n    Partition 14 : Follower\n    Partition 15 : Follower\n    Partition 16 : Follower\n  Broker 2 - 10.177.71.131:26501\n    Partition 1 : Follower\n    Partition 2 : Follower\n    Partition 3 : Leader\n    Partition 4 : Follower\n    Partition 5 : Follower\n    Partition 6 : Leader\n    Partition 7 : Follower\n    Partition 8 : Follower\n    Partition 9 : Leader\n    Partition 10 : Follower\n    Partition 11 : Follower\n    Partition 12 : Leader\n    Partition 13 : Follower\n    Partition 14 : Follower\n    Partition 15 : Leader\n    Partition 16 : Follower\n```\n\n**Josh Wulf**: Too many partitions\n\n**Josh Wulf**: With three brokers you want to have three partitions\n\n**Josh Wulf**: More partitions == more work\n\n**Josh Wulf**: Which is cool if you have more brokers, because then they can each get some\n\n**Josh Wulf**: Broker 1 is not doing any processing\n\n**Josh Wulf**: two of the nodes are doing all the work\n\n**Josh Wulf**: and they are having to manage 16 partitions while they do it\n\n**Josh Wulf**: too much overhead\n\n**Josh Wulf**: I would start there: 3 partitions, try it again, see where it fails\n\n**Josh Wulf**: then restart the brokers in TRACE log level and do it again, and study the logs\n\n**Josh Wulf**: and turn on Prometheus metrics and start building graphs to see where the resource pressure is on each machine and in the system\n\n**Sarath Kumar**: Just for clarification, will the partition count affects the request count.\n\n**Josh Wulf**: Also, this: <https://zeebe.io/blog/2019/12/zeebe-performance-profiling/>\n\n**Josh Wulf**: It will affect the amount of meta-work that your brokers are doing\n\n**Josh Wulf**: managing a partition is not servicing requests, so they become resource starved\n\n**Sarath Kumar**: Yeah sure, i will reduce the partition, but this will reduce the processing speed right?\nWe have already done the prometheus setting, if possible sharing me the metric to calculate resource pressure\n\n**Josh Wulf**: How much? I don’t know. Measure it and let me know - I’m interested to know\n\n**Josh Wulf**: How do more partitions increase the processing speed?\n\n**Josh Wulf**: They do if they are spread across more brokers\n\n**Josh Wulf**: If you had 16 partitions and 16 brokers then of course it will be faster\n\n**Josh Wulf**: But if you have 3 men, how does giving them 16 jobs get your house built faster?\n\n**Josh Wulf**: You need 16 people doing 16 jobs for that to be effective\n\n**Sarath Kumar**: oh okay sure, will try this configuration.\nAnd what\'s the max parallel request count will gateway can handle?\n\n**Josh Wulf**: At the moment you have two people doing 16 jobs, and one - Broker 1, watching them\n\n**Josh Wulf**: It depends\n\n**Josh Wulf**: Put it on a Raspberry Pi like @Kristof Jozsa and you’ll get one metric\n\n**Josh Wulf**: Put it on a 16-CPU machine with 256GB RAM and you’ll get another one\n\n**Josh Wulf**: The only way to know is to measure and adjust parameters\n\n**Josh Wulf**: That’s what you are doing now - science\n\n**Sarath Kumar**: yeah sure, we will give a try on this configurations\n\n**Josh Wulf**: devise experiments, take notes, make hypotheses\n\n**Josh Wulf**: That’s how Mendel discovered genetic inheritance. Careful observation and experimentation\n\n**Josh Wulf**: And read that blog post - it is exactly about how to do this - performance profiling\n\n**Sarath Kumar**: Yup sure !!\n\n**Josh Wulf**: I would run it with partition counts from 3 to 5, replication 0, 2, and 3\n\n**Josh Wulf**: That gives you a matrix\n\n**Josh Wulf**: That’s nine combinations\n\n**Josh Wulf**: Measure throughput, end-to-end latency and mean workflows to failure\n\n**Josh Wulf**: Then do the same thing again with different sized instances\n\n**Josh Wulf**: I’d automate that. because if you try two different instance sizes, that’s already 18 runs\n\n**Josh Wulf**: and if you automate it, you can just as easily do three sizes for 27 runs\n\n**Josh Wulf**: And then when a new version comes out, before you deploy it, you run the same test suite again, and BAM! You’re a pro\n\n**Josh Wulf**: Then you give a talk at a conference and get hired by IBM\n\n**Josh Wulf**: I saw two guys talk at <http://DevConf.CZ|DevConf.CZ> last week\n\n**Josh Wulf**: They spent six months profiling databases on Cloud providers\n\n**Josh Wulf**: <https://twitter.com/sitapati/status/1221389377340956672>\n\n**Sarath Kumar**: oh wow, well motivated to complete this :sunglasses:\n\n**Josh Wulf**: <https://twitter.com/sitapati/status/1221399049233928193>\n\n**Josh Wulf**: You got this\n\n**Josh Wulf**: @archivist2 How do I performance profile Zeebe for my use case?';
